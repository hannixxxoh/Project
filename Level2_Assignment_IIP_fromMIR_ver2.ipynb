{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DJvoIiAUj6P"
      },
      "source": [
        "# Music Auto-tagging Model\n",
        "- In this assignment, you will train your auto-tagging model using PyTorch\n",
        "- The dataset is from **MagnaTagATune**\n",
        "  - Randomly selected 8000 mp3 files\n",
        "  - 5000 files for training, 1000 for validation, 2000 for test  \n",
        "    - **주의**: 메모리가 부족하거나 학습이 오래 걸릴 경우 dataset 함수 load 시 num_max_data를 줄여서 해결\n",
        " \n",
        "- Problem 1: Complete Three Dataset Classes\n",
        "- Problem 2: Practice with nn.Sequential()\n",
        "- Problem 3: Make Your Own Conv Layers\n",
        "- Probelm 4: Try Various Settings and Report\n",
        "- Problem 5: Complete Binary Cross Entropy Function\n",
        "- Problem 6: Complete Precision-Recall Area Under Curve Function\n",
        "- Problem 7: Load audio and make prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHzI6RgPwhqc",
        "outputId": "543092ae-077c-470f-9cb2-8c75a1bd26c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://archive.ubuntu.com/ubuntu focal InRelease\n",
            "\r0% [Waiting for headers] [Connecting to cloud.r-project.org (65.9.86.109)] [Con\r                                                                               \rGet:2 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n",
            "\r                                                                               \rGet:3 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n",
            "\r                                                                               \rGet:4 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
            "\r                                                                               \rGet:5 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease [3,622 B]\n",
            "\r                                                                               \rGet:6 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease [18.1 kB]\n",
            "\r                                                                               \rHit:7 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease\n",
            "\r                                                                               \rGet:8 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease [18.1 kB]\n",
            "Hit:9 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease\n",
            "Get:10 http://ppa.launchpad.net/savoury1/ffmpeg4/ubuntu focal InRelease [18.1 kB]\n",
            "Ign:11 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu2004/x86_64  InRelease\n",
            "Hit:12 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\n",
            "Hit:13 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu2004/x86_64  Release\n",
            "Get:14 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [1,989 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [2,899 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1,281 kB]\n",
            "Get:17 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal/main Sources [2,374 kB]\n",
            "Get:18 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal/main amd64 Packages [1,125 kB]\n",
            "Get:19 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal/main amd64 Packages [29.5 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [1,864 kB]\n",
            "Get:21 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [980 kB]\n",
            "Get:22 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [2,425 kB]\n",
            "Get:23 http://ppa.launchpad.net/savoury1/ffmpeg4/ubuntu focal/main amd64 Packages [98.7 kB]\n",
            "Fetched 15.5 MB in 7s (2,074 kB/s)\n",
            "Reading package lists... Done\n",
            "Extracting templates from packages: 100%\n",
            "(Reading database ... 129501 files and directories currently installed.)\n",
            "Removing libopencv-dev (4.2.0+dfsg-5) ...\n",
            "Removing libopencv-contrib-dev:amd64 (4.2.0+dfsg-5) ...\n",
            "Removing libopencv-superres-dev:amd64 (4.2.0+dfsg-5) ...\n",
            "Removing libopencv-stitching-dev:amd64 (4.2.0+dfsg-5) ...\n",
            "Removing libopencv-objdetect-dev:amd64 (4.2.0+dfsg-5) ...\n",
            "dpkg: libavresample-dev:amd64: dependency problems, but removing anyway as you requested:\n",
            " libopencv-videoio-dev:amd64 depends on libavresample-dev.\n",
            " libopencv-highgui-dev:amd64 depends on libavresample-dev.\n",
            "\n",
            "Removing libavresample-dev:amd64 (7:4.2.7-0ubuntu0.1) ...\n",
            "dpkg: libopencv-videoio-dev:amd64: dependency problems, but removing anyway as you requested:\n",
            " libopencv-highgui-dev:amd64 depends on libopencv-videoio-dev (= 4.2.0+dfsg-5); however:\n",
            "  Package libopencv-videoio-dev:amd64 is to be removed.\n",
            "\n",
            "Removing libopencv-videoio-dev:amd64 (4.2.0+dfsg-5) ...\n",
            "dpkg: libopencv-highgui-dev:amd64: dependency problems, but removing anyway as you requested:\n",
            " libopencv-features2d-dev:amd64 depends on libopencv-highgui-dev (= 4.2.0+dfsg-5).\n",
            "\n",
            "Removing libopencv-highgui-dev:amd64 (4.2.0+dfsg-5) ...\n",
            "(Reading database ... 129148 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libavformat-dev_7%3a4.4.3-0ubuntu1~20.04.sav1_amd64.deb ...\n",
            "Unpacking libavformat-dev:amd64 (7:4.4.3-0ubuntu1~20.04.sav1) over (7:4.2.7-0ubuntu0.1) ...\n",
            "Preparing to unpack .../01-libavcodec-dev_7%3a4.4.3-0ubuntu1~20.04.sav1_amd64.deb ...\n",
            "Unpacking libavcodec-dev:amd64 (7:4.4.3-0ubuntu1~20.04.sav1) over (7:4.2.7-0ubuntu0.1) ...\n",
            "Preparing to unpack .../02-libswresample-dev_7%3a4.4.3-0ubuntu1~20.04.sav1_amd64.deb ...\n",
            "Unpacking libswresample-dev:amd64 (7:4.4.3-0ubuntu1~20.04.sav1) over (7:4.2.7-0ubuntu0.1) ...\n",
            "Preparing to unpack .../03-ffmpeg_7%3a4.4.3-0ubuntu1~20.04.sav1_amd64.deb ...\n",
            "Unpacking ffmpeg (7:4.4.3-0ubuntu1~20.04.sav1) over (7:4.2.7-0ubuntu0.1) ...\n",
            "Preparing to unpack .../04-libavdevice58_7%3a4.4.3-0ubuntu1~20.04.sav1_amd64.deb ...\n",
            "Unpacking libavdevice58:amd64 (7:4.4.3-0ubuntu1~20.04.sav1) over (7:4.2.7-0ubuntu0.1) ...\n",
            "Preparing to unpack .../05-libavfilter7_7%3a4.4.3-0ubuntu1~20.04.sav1_amd64.deb ...\n",
            "Unpacking libavfilter7:amd64 (7:4.4.3-0ubuntu1~20.04.sav1) over (7:4.2.7-0ubuntu0.1) ...\n",
            "Preparing to unpack .../06-libavformat58_7%3a4.4.3-0ubuntu1~20.04.sav1_amd64.deb ...\n",
            "Unpacking libavformat58:amd64 (7:4.4.3-0ubuntu1~20.04.sav1) over (7:4.2.7-0ubuntu0.1) ...\n",
            "Preparing to unpack .../07-libavcodec58_7%3a4.4.3-0ubuntu1~20.04.sav1_amd64.deb ...\n",
            "Unpacking libavcodec58:amd64 (7:4.4.3-0ubuntu1~20.04.sav1) over (7:4.2.7-0ubuntu0.1) ...\n",
            "Preparing to unpack .../08-libswresample3_7%3a4.4.3-0ubuntu1~20.04.sav1_amd64.deb ...\n",
            "Unpacking libswresample3:amd64 (7:4.4.3-0ubuntu1~20.04.sav1) over (7:4.2.7-0ubuntu0.1) ...\n",
            "Preparing to unpack .../09-libpostproc55_7%3a4.4.3-0ubuntu1~20.04.sav1_amd64.deb ...\n",
            "Unpacking libpostproc55:amd64 (7:4.4.3-0ubuntu1~20.04.sav1) over (7:4.2.7-0ubuntu0.1) ...\n",
            "Preparing to unpack .../10-libva-drm2_2.17.0-1~20.04.sav0_amd64.deb ...\n",
            "Unpacking libva-drm2:amd64 (2.17.0-1~20.04.sav0) over (2.7.0-2) ...\n",
            "Preparing to unpack .../11-libva-x11-2_2.17.0-1~20.04.sav0_amd64.deb ...\n",
            "Unpacking libva-x11-2:amd64 (2.17.0-1~20.04.sav0) over (2.7.0-2) ...\n",
            "Preparing to unpack .../12-libva2_2.17.0-1~20.04.sav0_amd64.deb ...\n",
            "Unpacking libva2:amd64 (2.17.0-1~20.04.sav0) over (2.7.0-2) ...\n",
            "Selecting previously unselected package libmfx1:amd64.\n",
            "Preparing to unpack .../13-libmfx1_22.4.4-0ubuntu1~20.04.sav0_amd64.deb ...\n",
            "Unpacking libmfx1:amd64 (22.4.4-0ubuntu1~20.04.sav0) ...\n",
            "Preparing to unpack .../14-libswscale-dev_7%3a4.4.3-0ubuntu1~20.04.sav1_amd64.deb ...\n",
            "Unpacking libswscale-dev:amd64 (7:4.4.3-0ubuntu1~20.04.sav1) over (7:4.2.7-0ubuntu0.1) ...\n",
            "Preparing to unpack .../15-libswscale5_7%3a4.4.3-0ubuntu1~20.04.sav1_amd64.deb ...\n",
            "Unpacking libswscale5:amd64 (7:4.4.3-0ubuntu1~20.04.sav1) over (7:4.2.7-0ubuntu0.1) ...\n",
            "Preparing to unpack .../16-libavutil-dev_7%3a4.4.3-0ubuntu1~20.04.sav1_amd64.deb ...\n",
            "Unpacking libavutil-dev:amd64 (7:4.4.3-0ubuntu1~20.04.sav1) over (7:4.2.7-0ubuntu0.1) ...\n",
            "(Reading database ... 129182 files and directories currently installed.)\n",
            "Removing libavresample4:amd64 (7:4.2.7-0ubuntu0.1) ...\n",
            "(Reading database ... 129176 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libavutil56_7%3a4.4.3-0ubuntu1~20.04.sav1_amd64.deb ...\n",
            "Unpacking libavutil56:amd64 (7:4.4.3-0ubuntu1~20.04.sav1) over (7:4.2.7-0ubuntu0.1) ...\n",
            "Selecting previously unselected package libdecor-0-0:amd64.\n",
            "Preparing to unpack .../01-libdecor-0-0_0.1.0-2~20.04.sav0_amd64.deb ...\n",
            "Unpacking libdecor-0-0:amd64 (0.1.0-2~20.04.sav0) ...\n",
            "Preparing to unpack .../02-libsdl2-2.0-0_2.0.22+dfsg-6.1~20.04.sav2_amd64.deb ...\n",
            "Unpacking libsdl2-2.0-0:amd64 (2.0.22+dfsg-6.1~20.04.sav2) over (2.0.10+dfsg1-3) ...\n",
            "Preparing to unpack .../03-libsndio7.0_1.9.0-0.3~20.04.sav0_amd64.deb ...\n",
            "Unpacking libsndio7.0:amd64 (1.9.0-0.3~20.04.sav0) over (1.5.0-3) ...\n",
            "Selecting previously unselected package libsndio7:amd64.\n",
            "Preparing to unpack .../04-libsndio7_1.9.0-0.3~20.04.sav0_amd64.deb ...\n",
            "Unpacking libsndio7:amd64 (1.9.0-0.3~20.04.sav0) ...\n",
            "Selecting previously unselected package libsphinxbase3:amd64.\n",
            "Preparing to unpack .../05-libsphinxbase3_0.8+5prealpha+1-8_amd64.deb ...\n",
            "Unpacking libsphinxbase3:amd64 (0.8+5prealpha+1-8) ...\n",
            "Selecting previously unselected package libpocketsphinx3:amd64.\n",
            "Preparing to unpack .../06-libpocketsphinx3_0.8.0+real5prealpha+1-6ubuntu4_amd64.deb ...\n",
            "Unpacking libpocketsphinx3:amd64 (0.8.0+real5prealpha+1-6ubuntu4) ...\n",
            "Preparing to unpack .../07-librubberband2_3.0.0+dfsg0-1~20.04.sav1_amd64.deb ...\n",
            "Unpacking librubberband2:amd64 (3.0.0+dfsg0-1~20.04.sav1) over (1.8.2-1build1) ...\n",
            "Selecting previously unselected package libvmaf1.\n",
            "Preparing to unpack .../08-libvmaf1_2.3.1-0ubuntu1~20.04.sav0_amd64.deb ...\n",
            "Unpacking libvmaf1 (2.3.1-0ubuntu1~20.04.sav0) ...\n",
            "Selecting previously unselected package libzimg2:amd64.\n",
            "Preparing to unpack .../09-libzimg2_3.0.4+ds1-1~20.04.sav0_amd64.deb ...\n",
            "Unpacking libzimg2:amd64 (3.0.4+ds1-1~20.04.sav0) ...\n",
            "Selecting previously unselected package libnettle8:amd64.\n",
            "Preparing to unpack .../10-libnettle8_3.7.3-1~20.04.sav0_amd64.deb ...\n",
            "Unpacking libnettle8:amd64 (3.7.3-1~20.04.sav0) ...\n",
            "Setting up libnettle8:amd64 (3.7.3-1~20.04.sav0) ...\n",
            "Selecting previously unselected package libhogweed6:amd64.\n",
            "(Reading database ... 129215 files and directories currently installed.)\n",
            "Preparing to unpack .../libhogweed6_3.7.3-1~20.04.sav0_amd64.deb ...\n",
            "Unpacking libhogweed6:amd64 (3.7.3-1~20.04.sav0) ...\n",
            "Setting up libhogweed6:amd64 (3.7.3-1~20.04.sav0) ...\n",
            "(Reading database ... 129220 files and directories currently installed.)\n",
            "Preparing to unpack .../libgnutls30_3.7.3-4ubuntu1.1~20.04.sav0_amd64.deb ...\n",
            "Unpacking libgnutls30:amd64 (3.7.3-4ubuntu1.1~20.04.sav0) over (3.6.13-2ubuntu1.7) ...\n",
            "Setting up libgnutls30:amd64 (3.7.3-4ubuntu1.1~20.04.sav0) ...\n",
            "Selecting previously unselected package librabbitmq4:amd64.\n",
            "(Reading database ... 129236 files and directories currently installed.)\n",
            "Preparing to unpack .../00-librabbitmq4_0.10.0-1_amd64.deb ...\n",
            "Unpacking librabbitmq4:amd64 (0.10.0-1) ...\n",
            "Selecting previously unselected package libcjson1:amd64.\n",
            "Preparing to unpack .../01-libcjson1_1.7.15-1~20.04.sav0_amd64.deb ...\n",
            "Unpacking libcjson1:amd64 (1.7.15-1~20.04.sav0) ...\n",
            "Selecting previously unselected package libmbedcrypto7:amd64.\n",
            "Preparing to unpack .../02-libmbedcrypto7_2.28.2-1~20.04.sav0_amd64.deb ...\n",
            "Unpacking libmbedcrypto7:amd64 (2.28.2-1~20.04.sav0) ...\n",
            "Selecting previously unselected package librist4:amd64.\n",
            "Preparing to unpack .../03-librist4_0.2.7+dfsg-1~20.04.sav0_amd64.deb ...\n",
            "Unpacking librist4:amd64 (0.2.7+dfsg-1~20.04.sav0) ...\n",
            "Selecting previously unselected package libsrt1.5-gnutls:amd64.\n",
            "Preparing to unpack .../04-libsrt1.5-gnutls_1.5.1-1~20.04.sav0_amd64.deb ...\n",
            "Unpacking libsrt1.5-gnutls:amd64 (1.5.1-1~20.04.sav0) ...\n",
            "Selecting previously unselected package libaom3:amd64.\n",
            "Preparing to unpack .../05-libaom3_3.5.0-1~20.04.sav0_amd64.deb ...\n",
            "Unpacking libaom3:amd64 (3.5.0-1~20.04.sav0) ...\n",
            "Selecting previously unselected package libcodec2-1.0:amd64.\n",
            "Preparing to unpack .../06-libcodec2-1.0_1.0.5-1ubuntu2~20.04.sav0_amd64.deb ...\n",
            "Unpacking libcodec2-1.0:amd64 (1.0.5-1ubuntu2~20.04.sav0) ...\n",
            "Selecting previously unselected package libcrystalhd3:amd64.\n",
            "Preparing to unpack .../07-libcrystalhd3_1%3a0.0~git20110715.fdd2f19-13build1_amd64.deb ...\n",
            "Unpacking libcrystalhd3:amd64 (1:0.0~git20110715.fdd2f19-13build1) ...\n",
            "Selecting previously unselected package libdav1d6:amd64.\n",
            "Preparing to unpack .../08-libdav1d6_1.0.0-1~20.04.sav0_amd64.deb ...\n",
            "Unpacking libdav1d6:amd64 (1.0.0-1~20.04.sav0) ...\n",
            "Selecting previously unselected package librav1e0:amd64.\n",
            "Preparing to unpack .../09-librav1e0_0.6.2-0ubuntu1~20.04.sav0_amd64.deb ...\n",
            "Unpacking librav1e0:amd64 (0.6.2-0ubuntu1~20.04.sav0) ...\n",
            "Preparing to unpack .../10-libsnappy1v5_1.1.9-2+20.04.sav1_amd64.deb ...\n",
            "Unpacking libsnappy1v5:amd64 (1.1.9-2+20.04.sav1) over (1.1.8-1build1) ...\n",
            "Selecting previously unselected package libsvtav1enc1:amd64.\n",
            "Preparing to unpack .../11-libsvtav1enc1_1.4.1+dfsg-0ubuntu1~20.04.sav0_amd64.deb ...\n",
            "Unpacking libsvtav1enc1:amd64 (1.4.1+dfsg-0ubuntu1~20.04.sav0) ...\n",
            "Selecting previously unselected package libvpx7:amd64.\n",
            "Preparing to unpack .../12-libvpx7_1.12.0-1ubuntu1~20.04.sav0_amd64.deb ...\n",
            "Unpacking libvpx7:amd64 (1.12.0-1ubuntu1~20.04.sav0) ...\n",
            "Selecting previously unselected package libwebp7:amd64.\n",
            "Preparing to unpack .../13-libwebp7_1.2.4-0ubuntu1~20.04.sav0_amd64.deb ...\n",
            "Unpacking libwebp7:amd64 (1.2.4-0ubuntu1~20.04.sav0) ...\n",
            "Selecting previously unselected package libx264-164:amd64.\n",
            "Preparing to unpack .../14-libx264-164_2%3a0.164.3101+gitb093bbe-0ubuntu1~20.04.sav0_amd64.deb ...\n",
            "Unpacking libx264-164:amd64 (2:0.164.3101+gitb093bbe-0ubuntu1~20.04.sav0) ...\n",
            "Selecting previously unselected package libx265-199:amd64.\n",
            "Preparing to unpack .../15-libx265-199_3.5-0ubuntu1~20.04.sav0_amd64.deb ...\n",
            "Unpacking libx265-199:amd64 (3.5-0ubuntu1~20.04.sav0) ...\n",
            "(Reading database ... 129313 files and directories currently installed.)\n",
            "Removing libopencv-videostab-dev:amd64 (4.2.0+dfsg-5) ...\n",
            "Removing libopencv-calib3d-dev:amd64 (4.2.0+dfsg-5) ...\n",
            "Removing libopencv-features2d-dev:amd64 (4.2.0+dfsg-5) ...\n",
            "Selecting previously unselected package libdecor-0-plugin-1-cairo:amd64.\n",
            "(Reading database ... 129270 files and directories currently installed.)\n",
            "Preparing to unpack .../libdecor-0-plugin-1-cairo_0.1.0-2~20.04.sav0_amd64.deb ...\n",
            "Unpacking libdecor-0-plugin-1-cairo:amd64 (0.1.0-2~20.04.sav0) ...\n",
            "Setting up libaom3:amd64 (3.5.0-1~20.04.sav0) ...\n",
            "Setting up librabbitmq4:amd64 (0.10.0-1) ...\n",
            "Setting up libcodec2-1.0:amd64 (1.0.5-1ubuntu2~20.04.sav0) ...\n",
            "Setting up libvmaf1 (2.3.1-0ubuntu1~20.04.sav0) ...\n",
            "Setting up libx264-164:amd64 (2:0.164.3101+gitb093bbe-0ubuntu1~20.04.sav0) ...\n",
            "Setting up libsphinxbase3:amd64 (0.8+5prealpha+1-8) ...\n",
            "Setting up libsvtav1enc1:amd64 (1.4.1+dfsg-0ubuntu1~20.04.sav0) ...\n",
            "Setting up libcrystalhd3:amd64 (1:0.0~git20110715.fdd2f19-13build1) ...\n",
            "Setting up libcjson1:amd64 (1.7.15-1~20.04.sav0) ...\n",
            "Setting up librav1e0:amd64 (0.6.2-0ubuntu1~20.04.sav0) ...\n",
            "Setting up libsnappy1v5:amd64 (1.1.9-2+20.04.sav1) ...\n",
            "Setting up libpocketsphinx3:amd64 (0.8.0+real5prealpha+1-6ubuntu4) ...\n",
            "Setting up libva2:amd64 (2.17.0-1~20.04.sav0) ...\n",
            "Setting up libmbedcrypto7:amd64 (2.28.2-1~20.04.sav0) ...\n",
            "Setting up libdav1d6:amd64 (1.0.0-1~20.04.sav0) ...\n",
            "Setting up libx265-199:amd64 (3.5-0ubuntu1~20.04.sav0) ...\n",
            "Setting up libwebp7:amd64 (1.2.4-0ubuntu1~20.04.sav0) ...\n",
            "Setting up librubberband2:amd64 (3.0.0+dfsg0-1~20.04.sav1) ...\n",
            "Setting up libsrt1.5-gnutls:amd64 (1.5.1-1~20.04.sav0) ...\n",
            "Setting up libva-drm2:amd64 (2.17.0-1~20.04.sav0) ...\n",
            "Setting up libdecor-0-0:amd64 (0.1.0-2~20.04.sav0) ...\n",
            "Setting up libzimg2:amd64 (3.0.4+ds1-1~20.04.sav0) ...\n",
            "Setting up libvpx7:amd64 (1.12.0-1ubuntu1~20.04.sav0) ...\n",
            "Setting up libsndio7:amd64 (1.9.0-0.3~20.04.sav0) ...\n",
            "Setting up libmfx1:amd64 (22.4.4-0ubuntu1~20.04.sav0) ...\n",
            "Setting up libsdl2-2.0-0:amd64 (2.0.22+dfsg-6.1~20.04.sav2) ...\n",
            "Setting up libva-x11-2:amd64 (2.17.0-1~20.04.sav0) ...\n",
            "Setting up libavutil56:amd64 (7:4.4.3-0ubuntu1~20.04.sav1) ...\n",
            "Setting up librist4:amd64 (0.2.7+dfsg-1~20.04.sav0) ...\n",
            "Setting up libdecor-0-plugin-1-cairo:amd64 (0.1.0-2~20.04.sav0) ...\n",
            "Setting up libpostproc55:amd64 (7:4.4.3-0ubuntu1~20.04.sav1) ...\n",
            "Setting up libsndio7.0:amd64 (1.9.0-0.3~20.04.sav0) ...\n",
            "Setting up libswscale5:amd64 (7:4.4.3-0ubuntu1~20.04.sav1) ...\n",
            "Setting up libavutil-dev:amd64 (7:4.4.3-0ubuntu1~20.04.sav1) ...\n",
            "Setting up libswresample3:amd64 (7:4.4.3-0ubuntu1~20.04.sav1) ...\n",
            "Setting up libswscale-dev:amd64 (7:4.4.3-0ubuntu1~20.04.sav1) ...\n",
            "Setting up libavcodec58:amd64 (7:4.4.3-0ubuntu1~20.04.sav1) ...\n",
            "Setting up libswresample-dev:amd64 (7:4.4.3-0ubuntu1~20.04.sav1) ...\n",
            "Setting up libavformat58:amd64 (7:4.4.3-0ubuntu1~20.04.sav1) ...\n",
            "Setting up libavcodec-dev:amd64 (7:4.4.3-0ubuntu1~20.04.sav1) ...\n",
            "Setting up libavformat-dev:amd64 (7:4.4.3-0ubuntu1~20.04.sav1) ...\n",
            "Setting up libavfilter7:amd64 (7:4.4.3-0ubuntu1~20.04.sav1) ...\n",
            "Setting up libavdevice58:amd64 (7:4.4.3-0ubuntu1~20.04.sav1) ...\n",
            "Setting up ffmpeg (7:4.4.3-0ubuntu1~20.04.sav1) ...\n",
            "Processing triggers for man-db (2.9.1-1) ...\n",
            "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n"
          ]
        }
      ],
      "source": [
        "!add-apt-repository -y ppa:savoury1/ffmpeg4\n",
        "!apt-get -qq install -y ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XENajfe8Uj6U"
      },
      "outputs": [],
      "source": [
        "DEV = 'cuda' # select your device 'cpu' or 'cuda'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKLjUzcbUj6W"
      },
      "source": [
        "## 0. Import Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "d5LcyB2rUj6W"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchaudio\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import IPython.display as ipd\n",
        "from datetime import datetime\n",
        "\n",
        "def save_fig_with_date(figname):\n",
        "  plt.savefig(f\"{figname}_{datetime.now().strftime('%m_%d_%H_%M_%S')}.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiocvB4v8jCC"
      },
      "source": [
        "- Download dataset from Google Drive link and Unzip at `MTAT_SMALL/`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "hEjKpTS_WCu4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29ba9738-d431-45a2-c4aa-eb6ae3560d5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.8/dist-packages (4.4.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.8/dist-packages (from gdown) (2.25.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from gdown) (3.9.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from gdown) (4.64.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (from gdown) (4.6.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (2022.12.7)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "/usr/local/lib/python3.8/dist-packages/gdown/cli.py:127: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Access denied with the following error:\n",
            "\n",
            " \tCannot retrieve the public link of the file. You may need to change\n",
            "\tthe permission to 'Anyone with the link', or have had many accesses. \n",
            "\n",
            "You may still be able to access the file from the browser:\n",
            "\n",
            "\t https://drive.google.com/uc?id=15e9E3oZdudErkPKwb0rCAiZXkPxdZkV6 \n",
            "\n",
            "unzip:  cannot find or open mtat_8000.zip, mtat_8000.zip.zip or mtat_8000.zip.ZIP.\n",
            "rm: cannot remove './mtat_8000.zip': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!pip install gdown\n",
        "!gdown --id 15e9E3oZdudErkPKwb0rCAiZXkPxdZkV6\n",
        "!unzip -q mtat_8000.zip\n",
        "!rm ./mtat_8000.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOSCuOJMUj6W"
      },
      "source": [
        "## Problem 1. Complete Dataset Class\n",
        "- In this problem, you have to implement three ways to load the data\n",
        "    - 1) Load audio file and resample every time the data is called **(OnTheFlyDataset)**\n",
        "        - 매번 파일을 불러와서 전처리하는 방식\n",
        "    - 2) Save pre-processed data in .pt file and load it every time the data is called **(PreProcessDataset)**\n",
        "        - 전처리한 파일을 하드디스크에 저장 후 불러오는 방식\n",
        "    - 3) Load every audio file on memory before the training starts **(OnMemoryDataset)**\n",
        "        - 메모리(RAM)에 저장 후 불러오는 방식"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "T_lpbZK1Uj6X"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "You don't have to change this cell\n",
        "'''\n",
        "class MTATDataset:\n",
        "  def __init__(self, dir_path, split='train', num_max_data=4000, sr=16000):\n",
        "    self.dir = Path(dir_path)\n",
        "    self.labels = pd.read_csv(self.dir / \"meta.csv\", index_col=[0])\n",
        "    self.sr = sr\n",
        "\n",
        "    if split==\"train\":\n",
        "      sub_dir_ids = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'a', 'b', 'c']\n",
        "    elif split=='valid':\n",
        "      sub_dir_ids = ['d']\n",
        "    else: #test\n",
        "      sub_dir_ids = ['e', 'f', 'g']\n",
        "\n",
        "    is_in_set = [True if x[0] in sub_dir_ids else False for x in self.labels['mp3_path'].values.astype('str')]\n",
        "    self.labels = self.labels.iloc[is_in_set]\n",
        "    self.labels = self.labels[:num_max_data]\n",
        "    self.vocab = self.labels.columns.values[1:-1]\n",
        "    self.label_tensor = self.convert_label_to_tensor()\n",
        "  \n",
        "  def convert_label_to_tensor(self):\n",
        "    return torch.LongTensor(self.labels.values[:, 1:-1].astype('bool'))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.labels)\n",
        "  \n",
        "\n",
        "MTAT_DIR = Path('MTAT_SMALL/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "hMLCf6oYwhqj",
        "outputId": "47f5a700-0493-4524-d5c6-f1056a8a5056"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-d60b5f210d6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m '''\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mbase_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMTATDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMTAT_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m '''\n",
            "\u001b[0;32m<ipython-input-5-af61e5cab4af>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dir_path, split, num_max_data, sr)\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdir_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_max_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdir\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"meta.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'MTAT_SMALL/meta.csv'"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Check how baseline dataset looks like\n",
        "'''\n",
        "\n",
        "base_set = MTATDataset(MTAT_DIR)\n",
        "\n",
        "'''\n",
        "metadata of dataset is stored in self.labels\n",
        "'''\n",
        "base_set.labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30qqk8Khwhqj"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "You can use labels['mp3_path'].iloc\n",
        "'''\n",
        "target_idx = 0 \n",
        "\n",
        "path_to_target_idx = base_set.labels['mp3_path'].iloc[target_idx]\n",
        "print(path_to_target_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "N3yRTkSHwhqj"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "label of each tensor is also stored in self.label_tensor\n",
        "'''\n",
        "base_set.label_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CwmKCROOwhqk"
      },
      "outputs": [],
      "source": [
        "class OnTheFlyDataset(MTATDataset):\n",
        "  def __init__(self, dir_path, split='train', num_max_data=4000, sr=16000):\n",
        "    super().__init__(dir_path, split, num_max_data, sr)\n",
        "    \n",
        "  def __getitem__(self, idx):\n",
        "    '''\n",
        "    __getitem__ returns a corresponding idx-th data sample among the dataset.\n",
        "    In music-tag dataset, it has to return (audio_sample, label) of idx-th data.\n",
        "    \n",
        "    OnTheFlyDataset loads the audio file whenever this __getitem__ function is called.\n",
        "    In this function, you have to implement these things\n",
        "    \n",
        "    1) Get the file path of idx-th data sample (use self.labels['mp3_path'])\n",
        "    2) Load the audio of that file path\n",
        "    3) Resample the audio sample into frequency of self.sr (You can use torchaudio.functional.resample)\n",
        "    4) Return resampled audio sample and the label (tag data) of the data sample\n",
        "    \n",
        "    Output\n",
        "      audio_sample (torch.FloatTensor):  \n",
        "      label (torch.FloatTensor): A tensor with shape of 50 dimension. Each dimension has value either 0 or 1\n",
        "                                 If n-th dimension's value is 1, it means n-th tag is True for this data sample\n",
        "    \n",
        "    TODO: Complete this function\n",
        "    '''\n",
        "    audio_sample = None\n",
        "    label = None\n",
        "    \n",
        "    return audio_sample, label\n",
        "\n",
        "dummy_set = OnTheFlyDataset(MTAT_DIR, split='train', num_max_data=100)\n",
        "audio, label = dummy_set[1]\n",
        "assert audio.ndim == 1, \"Number of dimensions of audio tensor has to be 1. Use audio[0] or audio.mean(dim=0) to reduce it\"\n",
        "ipd.display(ipd.Audio(audio, rate=dummy_set.sr))\n",
        "print(dummy_set.vocab[torch.where(label)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Z4OSo-pwhqk"
      },
      "outputs": [],
      "source": [
        "class PreProcessDataset(MTATDataset):\n",
        "  def __init__(self, dir_path, split='train', num_max_data=8000, sr=16000):\n",
        "    super().__init__(dir_path, split, num_max_data, sr)\n",
        "    \n",
        "    self.pre_process_and_save_data()\n",
        "    \n",
        "  def pre_process_and_save_data(self):\n",
        "    '''\n",
        "    self.pre_process_and_save_data loads every audio sample in the dataset, resample it, and save it into pt file.\n",
        "    In this function, you have to implement these things\n",
        "    \n",
        "    1) For every data sample in the dataset, check whether pre-processed data already exists\n",
        "      - You can get data sample path by self.labels['mp3_path'].values()\n",
        "      - path of pre-processed data can be in the same directory, but with different suffix.\n",
        "      - You can make it with Path(mp3_path).with_suffix('.pt')\n",
        "    2) If it doesn't exist, do follow things\n",
        "      a) Load audio file \n",
        "      b) Resample the audio file with samplerate of self.sr\n",
        "      c) Get label of this audio file\n",
        "      d) Save {'audio': audio_tensor, 'label':label_tensor} with torch.save\n",
        "    \n",
        "    Output\n",
        "      None\n",
        "    \n",
        "    TODO: Complete this function\n",
        "    '''\n",
        "    \n",
        "    \n",
        "  def __getitem__(self, idx):\n",
        "    '''\n",
        "    __getitem__ returns a corresponding idx-th data sample among the dataset.\n",
        "    In music-tag dataset, it has to return (audio_sample, label) of idx-th data.\n",
        "    \n",
        "    PreProcessDataset loads the pre-processed pt file whenever this __getitem__ function is called.\n",
        "    In this function, you have to implement these things\n",
        "    \n",
        "    1) Get the pt file path of idx-th data sample (use self.labels)\n",
        "    2) Load the pre-procssed data of that file path (use torch.load)\n",
        "    3) Return the audio sample and the label (tag data) of the data sample\n",
        "\n",
        "    TODO: Complete this function\n",
        "    '''\n",
        "\n",
        "    return\n",
        "  \n",
        "dummy_set = PreProcessDataset(MTAT_DIR, split='train', num_max_data=100)\n",
        "audio, label = dummy_set[15]\n",
        "assert audio.ndim == 1, \"Number of dimensions of audio tensor has to be 1. Use audio[0] or audio.mean(dim=0) to reduce it\"\n",
        "ipd.display(ipd.Audio(audio, rate=dummy_set.sr))\n",
        "print(dummy_set.vocab[torch.where(label)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "JTVjipu4whqk"
      },
      "outputs": [],
      "source": [
        "class OnMemoryDataset(MTATDataset):\n",
        "  def __init__(self, dir_path, split='train', num_max_data=4000, sr=16000):\n",
        "    super().__init__(dir_path, split, num_max_data, sr)\n",
        "    \n",
        "    self.loaded_audios = self.load_audio()\n",
        "    \n",
        "  def load_audio(self):\n",
        "    '''\n",
        "    In this function, you have to load all the audio file in the dataset, and resample them, \n",
        "    and store the data on the memory as a python variable\n",
        "    \n",
        "    For each data in the dataset,\n",
        "      a) Load Audio\n",
        "      b) Resample it to self.sr\n",
        "      c) Append it to total_audio_datas\n",
        "    \n",
        "    Output:\n",
        "      total_audio_datas (list): A list of torch.FloatTensor. i-th item of the list corresponds to the audio sample of i-th data\n",
        "                                Each item is an audio sample in torch.FloatTensor with sampling rate of self.sr \n",
        "    '''\n",
        "    total_audio_datas = []\n",
        "    \n",
        "    ### Write your code from here\n",
        "    return total_audio_datas\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    '''\n",
        "    __getitem__ returns a corresponding idx-th data sample among the dataset.\n",
        "    In music-tag dataset, it has to return (audio_sample, label) of idx-th data.\n",
        "    \n",
        "    OnMemoryDataset returns the pre-loaded audio data that is aved on self.loaded_audios whenever this __getitem__ function is called.\n",
        "    In this function, you have to implement these things\n",
        "    \n",
        "    1) Load the pre-procssed audio data from self.loaded_audios\n",
        "    2) Return the audio sample and the label (tag data) of the data sample\n",
        "\n",
        "    TODO: Complete this function\n",
        "    '''\n",
        "    \n",
        "    return\n",
        "  \n",
        "dummy_set = OnMemoryDataset(MTAT_DIR, split='train', num_max_data=50)\n",
        "audio, label = dummy_set[10]\n",
        "assert audio.ndim == 1, \"Number of dimensions of audio tensor has to be 1. Use audio[0] or audio.mean(dim=0) to reduce it\"\n",
        "ipd.display(ipd.Audio(audio, rate=dummy_set.sr))\n",
        "print(dummy_set.vocab[torch.where(label)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaVQGJbWwhql"
      },
      "source": [
        "#### Define Dataset\n",
        "- You can select one of your implementations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "AEhSHYC_whql"
      },
      "outputs": [],
      "source": [
        "your_dataset_class = PreProcessDataset # One of OnTheFlyDataset, PreProcessDataset, or OnMemoryDataset\n",
        "# your_dataset_class = OnMemoryDataset\n",
        "'''\n",
        "Based on your memory size or storage size, you can change the num_max_data\n",
        "코드 동작만 테스트하거나 컴퓨터 사양이 낮으면 num_max_data 줄여서 시도\n",
        "'''\n",
        "trainset = your_dataset_class(MTAT_DIR, split='train', num_max_data=5000)\n",
        "validset = your_dataset_class(MTAT_DIR, split='valid', num_max_data=1000)\n",
        "testset = your_dataset_class(MTAT_DIR, split='test', num_max_data=2000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIPU7DxUUj6Y"
      },
      "source": [
        "#### DataLoader\n",
        "- Define `DataLoader` using the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bPauowdkUj6Z"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(trainset, batch_size=64, shuffle=True, num_workers=0) # you can speed up with num_workers=4 if you have multiple cpu core\n",
        "valid_loader = DataLoader(validset, batch_size=128, shuffle=False, num_workers=0)\n",
        "test_loader = DataLoader(testset, batch_size=128, shuffle=False, num_workers=0)\n",
        "\n",
        "batch = next(iter(train_loader))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2kAXFFZUj6a"
      },
      "source": [
        "## Preparation: Define Neural Network\n",
        "- Define the neural network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TB0nTbt7Uj6a"
      },
      "outputs": [],
      "source": [
        "class SpecModel(nn.Module):\n",
        "  def __init__(self, sr, n_fft, hop_length, n_mels):\n",
        "    super().__init__()\n",
        "    self.mel_converter = torchaudio.transforms.MelSpectrogram(sample_rate=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels)\n",
        "    self.db_converter = torchaudio.transforms.AmplitudeToDB()\n",
        "  \n",
        "  def forward(self, x):\n",
        "    mel_spec = self.mel_converter(x)\n",
        "    return self.db_converter(mel_spec)\n",
        "\n",
        "class AudioModel(nn.Module):\n",
        "  def __init__(self, sr, n_fft, hop_length, n_mels, hidden_size, num_output):\n",
        "    super().__init__()\n",
        "    self.sr = sr\n",
        "    self.spec_converter = SpecModel(sr, n_fft, hop_length, n_mels)\n",
        "    self.conv_layer = nn.Sequential(\n",
        "      nn.Conv1d(n_mels, out_channels=hidden_size, kernel_size=3),\n",
        "      nn.MaxPool1d(3),\n",
        "      nn.ReLU(),\n",
        "      nn.Conv1d(hidden_size, out_channels=hidden_size, kernel_size=3),\n",
        "      nn.MaxPool1d(3),\n",
        "      nn.ReLU(),     \n",
        "      nn.Conv1d(hidden_size, out_channels=hidden_size, kernel_size=3),\n",
        "      nn.MaxPool1d(3),\n",
        "      nn.ReLU(),\n",
        "    )\n",
        "    self.final_layer = nn.Linear(hidden_size, num_output)\n",
        "\n",
        "  def get_spec(self, x):\n",
        "    '''\n",
        "    Get result of self.spec_converter\n",
        "    x (torch.Tensor): audio samples (num_batch_size X num_audio_samples)\n",
        "    '''\n",
        "    return self.spec_converter(x)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    spec = self.get_spec(x) # num_batch X num_mel_bins X num_time_bins\n",
        "    out = self.conv_layer(spec)\n",
        "    out = torch.max(out, dim=-1)[0] # select [0] because torch.max outputs tuple of (value, index)\n",
        "    out = self.final_layer(out)\n",
        "    out = torch.sigmoid(out)\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0UIl_uvUj6b"
      },
      "source": [
        "## 3. Train the Network\n",
        "- First, just run the cells below so that you can obtain the first result\n",
        "- Plot the training loss and validation accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6VBnZiX7Uj6b"
      },
      "outputs": [],
      "source": [
        "def get_tpr_fpr(pred, target, threshold=0.5):\n",
        "  thresh_pred = pred> threshold\n",
        "  p = torch.sum(target == 1)\n",
        "  tp = torch.sum((thresh_pred==1) * (target==1))\n",
        "  n = torch.sum(target == 0)\n",
        "  fp = torch.sum((thresh_pred==1) * (target==0))\n",
        "  return tp/p, fp/n\n",
        "\n",
        "def get_roc_auc(pred, label, num_grid=500):\n",
        "  auc = 0\n",
        "  prev_fpr = 0\n",
        "  for thresh in reversed(torch.linspace(0,1,num_grid)):\n",
        "    tpr, fpr = get_tpr_fpr(pred, label, threshold=thresh)\n",
        "    auc += tpr * (fpr-prev_fpr)\n",
        "    prev_fpr = fpr\n",
        "  return auc\n",
        "\n",
        "def train_model(model, train_loader, valid_loader, optimizer, num_epochs, loss_func, device='cuda'):\n",
        "  loss_records =[] \n",
        "  valid_acc_records = []\n",
        "  model.vocab = train_loader.dataset.vocab\n",
        "  model.train() # Set model to train mode\n",
        "  for epoch in tqdm(range(num_epochs)):\n",
        "    for batch in train_loader:\n",
        "      optimizer.zero_grad() # Rest gradient of every parameters in optimizer (every parameters in the model)\n",
        "      audio, label = batch\n",
        "      audio = audio.to(device)\n",
        "      label = label.to(device)\n",
        "      pred = model(audio)\n",
        "      loss = loss_func(pred, label.float())\n",
        "      loss.backward() # Run backpropagation\n",
        "      optimizer.step() # Update parameters\n",
        "      loss_records.append(loss.item())\n",
        "    valid_acc = validate_model(model, valid_loader, device)\n",
        "    valid_acc_records.append(valid_acc.item())\n",
        "  return {\"loss\": loss_records, \"valid_acc\": valid_acc_records}\n",
        "\n",
        "def validate_model(model, valid_loader, device, acc_func=get_roc_auc):\n",
        "  valid_acc = 0\n",
        "  model.eval()\n",
        "  model.to(device)\n",
        "  with torch.no_grad():\n",
        "    for batch in valid_loader:\n",
        "      audio, label = batch\n",
        "      pred = model(audio.to(device))\n",
        "      auc = acc_func(pred, label.to(device))\n",
        "      valid_acc += auc * len(label)\n",
        "  model.train()\n",
        "  return valid_acc / len(valid_loader.dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTHIVXvNUj6c",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Train the default model\n",
        "'''\n",
        "\n",
        "model = AudioModel(sr=16000, n_fft=1024, hop_length=512, n_mels=48, num_output=50, hidden_size=32)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "model = model.to(DEV)\n",
        "loss_func = torch.nn.BCELoss()\n",
        "train_record = train_model(model, train_loader, valid_loader, optimizer, num_epochs=30, loss_func=loss_func, device=DEV)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cUCYjyeyUj6c"
      },
      "outputs": [],
      "source": [
        "plt.plot(train_record['loss'])\n",
        "save_fig_with_date('default_train_loss')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJ92F5T1Uj6c"
      },
      "outputs": [],
      "source": [
        "plt.plot(train_record['valid_acc'])\n",
        "save_fig_with_date('default_train_valid_acc')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXnirpn4Uj6d"
      },
      "source": [
        "### Problem 2. Practice with nn.Sequential()\n",
        "- `nn.Sequential` automatically stacks the `nn.Module`\n",
        "    - If `x = nn.Sequential( nn.Conv1d(48, 12) , nn.ReLU() )`,\n",
        "        - `out = x(input)`\n",
        "        - `x1 = nn.Conv1d(48,12)`, `x2=nn.ReLU()`, `out = x2(x1(input))`\n",
        "        \n",
        "        위의 두 줄이 같은 동작\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgkwalESUj6d",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "class StackManualLayer(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.layer1 = nn.Conv1d(16, 4, kernel_size=2)\n",
        "    self.activation = nn.Sigmoid()\n",
        "    self.layer2 = nn.Conv1d(4, 4, kernel_size=2)\n",
        "    self.layer3 = nn.Conv1d(4, 1, kernel_size=2)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    out = self.layer1(x)\n",
        "    out = self.activation(out)\n",
        "    out = self.layer2(out)\n",
        "    out = self.activation(out)\n",
        "    out = self.layer3(out)\n",
        "    return out\n",
        "\n",
        "class SequentialLayer(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "      '''\n",
        "      TODO: Complete this nn.Sequential so that it computes exactly same thing with StackManualLayer\n",
        "      '''\n",
        "    )\n",
        "  def forward(self, x):\n",
        "    out = self.layers(x)\n",
        "    return out\n",
        "  \n",
        "# Do not change the code below\n",
        "torch.manual_seed(0)\n",
        "manual_layer = StackManualLayer()\n",
        "torch.manual_seed(0)\n",
        "sequential_layer = SequentialLayer()\n",
        "\n",
        "'''\n",
        "The printed result has to be same\n",
        "'''\n",
        "\n",
        "test_dummy = torch.arange(128).view(1,16,8).float()\n",
        "manual_out = manual_layer(test_dummy)\n",
        "print(f\"Output with Manual Stack Layer: {manual_out}\")\n",
        "sequential_out = sequential_layer(test_dummy)\n",
        "print(f\"Output with Sequential Layer: {sequential_out}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1O9g9pliUj6d"
      },
      "source": [
        "### Problem 3. Make Your Own Conv Layers\n",
        "- Complete the `self.conv_layer` of `YourModel`\n",
        "- Train the model and compare the result\n",
        "- In your report, explain your conv_layer and the training result\n",
        "    - How did you change the structure of CNN?\n",
        "    - What is the difference?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VIJnkBrRUj6e"
      },
      "outputs": [],
      "source": [
        "class YourModel(AudioModel):\n",
        "  def __init__(self, sr, n_fft, hop_length, n_mels, hidden_size, num_output):\n",
        "    super().__init__(sr, n_fft, hop_length, n_mels, hidden_size, num_output)\n",
        "    '''\n",
        "      TODO: Complete your new conv layer =\n",
        "      Example:\n",
        "      self.conv_layer = nn.Sequential(\n",
        "        nn.Conv1d(n_mels, out_channels=hidden_size, kernel_size=3),\n",
        "        nn.MaxPool1d(3),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv1d(hidden_size, out_channels=hidden_size, kernel_size=3),\n",
        "        nn.MaxPool1d(3),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "      '''\n",
        "    self.conv_layer = nn.Sequential(\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4DRtuzjUj6e"
      },
      "outputs": [],
      "source": [
        "your_model = YourModel(sr=16000, n_fft=1024, hop_length=512, n_mels=48, num_output=50, hidden_size=32)\n",
        "optimizer = torch.optim.Adam(your_model.parameters(), lr=1e-3)\n",
        "your_model = your_model.to(DEV)\n",
        "your_train_record = train_model(your_model, train_loader, valid_loader, optimizer, num_epochs=30, loss_func=loss_func, device=DEV)\n",
        "\n",
        "## Save the figure with comparison of default setting\n",
        "plt.figure(figsize=(8,16))\n",
        "plt.subplot(2,1,1)\n",
        "plt.plot(train_record['loss'])\n",
        "plt.plot(your_train_record['loss'])\n",
        "plt.subplot(2,1,2)\n",
        "plt.plot(train_record['valid_acc'])\n",
        "plt.plot(your_train_record['valid_acc'])\n",
        "save_fig_with_date('your_conv_layer_comparison_with_default')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yzROUqOUj6e"
      },
      "source": [
        "### Probelm 4. Try Various Settings and Report\n",
        "- You can try different `n_fft`, `n_mels`, or `hidden_size`, or different `conv_layer` in your model\n",
        "- Describe the result and your analysis in your report\n",
        "    - Why you tried those changes\n",
        "    - What you have expected from the result with those changes\n",
        "    - What you got from the changes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0h6XoqUUj6f"
      },
      "outputs": [],
      "source": [
        "your_model = YourModel(sr=16000, n_fft=1024, hop_length=512, n_mels=48, num_output=50, hidden_size=32)\n",
        "optimizer = torch.optim.Adam(your_model.parameters(), lr=1e-3)\n",
        "your_model = your_model.to(DEV)\n",
        "your_train_record = train_model(your_model, train_loader, valid_loader, optimizer, num_epochs=30, loss_func=loss_func, device=DEV)\n",
        "\n",
        "## Save the figure with comparison of default setting\n",
        "plt.figure(figsize=(8,16))\n",
        "plt.subplot(2,1,1)\n",
        "plt.plot(train_record['loss'])\n",
        "plt.plot(your_train_record['loss'])\n",
        "plt.subplot(2,1,2)\n",
        "plt.plot(train_record['valid_acc'])\n",
        "plt.plot(your_train_record['valid_acc'])\n",
        "save_fig_with_date('comparison_with_default')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CrO_pgA4whqo"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Get the test result\n",
        "'''\n",
        "test_acc = validate_model(your_model, test_loader, DEV)\n",
        "print(f\"Calculated ROC_AUC value for Test Set is : {test_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_PPNwM1Uj6f"
      },
      "source": [
        "### Problem 5 Complete Binary Cross Entropy Function\n",
        "- Complete the function that can calculate the Binary Cross Entropy for given prediction and target label without using `torch.BCELoss`\n",
        "- yi, yi_hat 순서 조심\n",
        "- ![bce](https://androidkt.com/wp-content/uploads/2021/05/Selection_099.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWhH13CEUj6f"
      },
      "outputs": [],
      "source": [
        "def get_binary_cross_entropy(pred:torch.Tensor, target:torch.Tensor):\n",
        "  '''\n",
        "  pred (torch.Tensor): predicted value of a neural network model for a given input (assume that the value is output of sigmoid function)\n",
        "  target (torch.Tensor): ground-truth label for a given input, given in multi-hot encoding\n",
        "\n",
        "  output (torch.Tensor): Mean Binary Cross Entropy Loss value of every sample\n",
        "  '''\n",
        "  # TODO: Complete this function\n",
        "  return \n",
        "\n",
        "test_model = AudioModel(sr=16000, n_fft=1024, hop_length=512, n_mels=48, num_output=50, hidden_size=16)\n",
        "test_model = test_model.to(DEV)\n",
        "test_optimizer = torch.optim.Adam(test_model.parameters(), lr=1e-3)\n",
        "train_record = train_model(test_model, train_loader, valid_loader, test_optimizer, num_epochs=5, loss_func=get_binary_cross_entropy, device=DEV)\n",
        "plt.subplot(2,1,1)\n",
        "plt.plot(train_record['loss'])\n",
        "plt.subplot(2,1,2)\n",
        "plt.plot(train_record['valid_acc'])\n",
        "save_fig_with_date('handmade_bce_result')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHpefphaUj6g"
      },
      "source": [
        "### Problem 6. Complete Precision-Recall Area Under Curve Function\n",
        "- One of the frequently used metric is Precision-Recall Area Under Curve (PR-AUC)\n",
        "- Precision is (Number of true positive)/(Number of total positive predictions)\n",
        "- Recall is (Number of true positive)/(Number of total positive ground-truth)\n",
        "- Precision and recall values depend on threshold\n",
        "- PR-AUC is the area under precision-recall curve of varying trheshold\n",
        "- You can refer the pre-defined `get_roc_auc` function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SS9LLkWnUj6g"
      },
      "outputs": [],
      "source": [
        "def get_precision_and_recall(pred:torch.Tensor, target:torch.Tensor, threshold:float):\n",
        "  '''\n",
        "  This function calculates precision and recall of given (prediction, target, threshold)\n",
        "  \n",
        "  pred (torch.Tensor): predicted value of a neural network model for a given input \n",
        "  target (torch.Tensor): ground-truth label for a given input, given in multi-hot encoding\n",
        "\n",
        "  output\n",
        "    precision (torch.Tensor): (Number of true positive)/(Number of total positive predictions)\n",
        "    recall (torch.Tensor): (Number of true positive)/(Number of total positive ground-truth)\n",
        "    \n",
        "  IMPORTANT:\n",
        "    To prevent division by zero, make the denominator greater than zero.\n",
        "  \n",
        "  TODO: Complete this function\n",
        "  '''\n",
        "  \n",
        "  # Write your code here\n",
        "  \n",
        "  precision = None\n",
        "  recall = None\n",
        "  \n",
        "  \n",
        "  '''\n",
        "  Be careful for not returning nan because of division by zero\n",
        "  '''\n",
        "  assert not (torch.isnan(precision) or torch.isnan(recall))\n",
        "  return precision, recall\n",
        "\n",
        "def get_precision_recall_auc(pred:torch.Tensor, target:torch.Tensor, num_grid=500):\n",
        "  '''\n",
        "  This function returns PR_AUC value for a given prediction and target.\n",
        "  Assume pred.shape == target.shape\n",
        "  \n",
        "  pred (torch.Tensor): predicted value of a neural network model for a given input \n",
        "  target (torch.Tensor): ground-truth label for a given input, given in multi-hot encoding\n",
        "\n",
        "  output (torch.Tensor): Area Under Curve value for Precision-Recall Curve\n",
        "  \n",
        "  TODO: Complete this function using get_precision_and_recall\n",
        "  '''\n",
        "\n",
        "  return\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1Z0UyTGwhqp"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Test the get_precision_recall_auc\n",
        "dummy 변수를 이용한 테스트\n",
        "'''\n",
        "\n",
        "dummy_pred = torch.Tensor([0.0285,     0.0004,     0.0483,     0.0003,     0.0074,     0.0141,\n",
        "            0.0007,     0.0735,     0.0534,     0.0153,     0.0024,     0.0053,\n",
        "            0.0004,     0.1033,     0.1007,     0.4314,     0.1744,     0.0119,\n",
        "            0.0189,     0.0075,     0.0001,     0.1354,     0.0014,     0.0004,\n",
        "            0.4431,     0.0236,     0.0005,     0.1276,     0.0173,     0.0000,\n",
        "            0.0010,     0.1237,     0.0616,     0.1674,     0.0000,     0.0053,\n",
        "            0.0984,     0.0608,     0.1783,     0.0689,     0.0509,     0.0011,\n",
        "            0.0749,     0.0001,     0.6105,     0.0136,     0.2644,     0.0204,\n",
        "            0.0005,     0.0001])\n",
        "dummy_target = torch.Tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
        "        0, 0])\n",
        "\n",
        "'''\n",
        "Printed result of code below has to be tensor(0.1753)\n",
        "'''\n",
        "get_precision_recall_auc(dummy_pred, dummy_target) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jaZdQDmhwhqq"
      },
      "outputs": [],
      "source": [
        "selected_model = model\n",
        "pr_auc_value_valid = validate_model(selected_model, valid_loader, DEV, acc_func=get_precision_recall_auc)\n",
        "pr_auc_value_test = validate_model(selected_model, test_loader, DEV, acc_func=get_precision_recall_auc)\n",
        "print(f\"Calculated PR_AUC value for Validation Set is : {pr_auc_value_valid.item():.4f}\")\n",
        "print(f\"Calculated PR_AUC value for Test Set is : {pr_auc_value_test.item():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93nJtVppUj6g"
      },
      "source": [
        "### Problem 7: Load audio and make prediction\n",
        "- Upload mp3 file of your choice\n",
        "    - If you are using Colab, you can upload file by opening File Browser at the sidebar\n",
        "- Try several audio files and report the result by comparing your expectation and model's output\n",
        "- You can get different result by modifying `THRESHOLD`\n",
        "    - `THRESHOLD` has to be a value between 0 and 1\n",
        "    - If you lower the `THRESHOLD`, more tags will be printed out\n",
        "- Complete `slice_audio` function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fMCcJNGUj6g"
      },
      "outputs": [],
      "source": [
        "your_audio_path = 'your_audio_file_path' #TODO\n",
        "selected_model = model # Change it if you want to select model with different name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jWXlws--Uj6h"
      },
      "outputs": [],
      "source": [
        "def get_resampled_mono_audio_from_file(audio_file_path, target_sr):\n",
        "  y, sr = torchaudio.load(audio_file_path)\n",
        "  if sr!= selected_model.sr:\n",
        "    y = torchaudio.functional.resample(y, orig_freq=sr, new_freq=selected_model.sr)\n",
        "  if y.shape[0] > 1:\n",
        "    y = torch.sum(y, dim=0) / y.shape[0]\n",
        "  \n",
        "  return y\n",
        "\n",
        "def slice_audio(audio_sample, sr, start_sec, end_sec):\n",
        "  '''\n",
        "  This function takes an audio sample, sampling rate, and start/end position of slice\n",
        "  and returns the sliced audio sample.\n",
        "  \n",
        "  audio_sample (torch.Tensor): A sequence of audio samples in shape of (N,), where N is number of audio samples\n",
        "  sr (int): Sampling rate of audio_sample\n",
        "  start_sec (float): desired slice start position in seconds\n",
        "  end_sec (float): desired slice end position in seconds\n",
        "  \n",
        "  output (torch.Tensor): A sequence of audio samples in shape of (int(sr*(end_sec-start_sec)), ) \n",
        "  \n",
        "  \n",
        "  TODO: Complete this function\n",
        "  '''\n",
        "  return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqzRixHZUj6h"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Run Model\n",
        "'''\n",
        "\n",
        "\n",
        "THRESHOLD = 0.2\n",
        "y = get_resampled_mono_audio_from_file(your_audio_path, selected_model.sr)\n",
        "\n",
        "'''\n",
        "You can slice your desired position\n",
        "'''\n",
        "sliced_y = slice_audio(y, selected_model.sr, 0, 30) \n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "  pred = selected_model(sliced_y.unsqueeze(0).to(DEV)).to('cpu')\n",
        "pred = pred[0]\n",
        "ipd.display(ipd.Audio(sliced_y, rate=selected_model.sr))\n",
        "print(f\"Predicted tags are: {model.vocab[torch.where(pred>THRESHOLD)]}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "CT",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6 | packaged by conda-forge | (main, Oct  7 2022, 20:14:50) [MSC v.1916 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "43ae48d8deb617ec67086938a96d28f2840d52524d48a3e6c25bb245dd21cd4f"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}